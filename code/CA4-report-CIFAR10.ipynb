{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI-CA4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### grayscale\n",
    "####  در مقابل رنگی بودن تصویر است. در این تکنیک سه کانال رنگی عکس را به یک کانال تبدیل می کنیم و عکس را خاکستری می کنیم.در اینجا پیچیدگی دیتا ست کاهش می یابد.با این کار  در عکس ها تاثیر رنگ در تصمیم گیری از بین می رود و لیبل عکس ها از روی ساختار اشیا تهیه می شود که در اینجا حجم دیتای تمرین کاهش یافته و این کاهش در تصمیم گیری می تواند کمک کننده باشد.\n",
    "#### PCA\n",
    "#### در این تکنیک از تعداد بعد های دیتای خود می کاهیم به طوری که بعد های باقی مانده توانایی تمایز بخشی دیتای اولیه را داشته باشد. در اینجا بین دقت و کاهش حجم داده یک رقابت است که باید یک نقطه بهینه بدست بیاید.\n",
    "#### random projection\n",
    "#### KNN یک تکنیک برای کاهش ابعاد داده است برای زمانی که به محاسبه سریع فاصله جفت جفت داده ها از هم نیاز است مثل \n",
    "#### در اینجا زمان محاسبات کمتر می شودو داده ها با بعد کمتر تمایز بیشتری نسبت به هم دارند و تصمیم گیری بر اساس اختلاف های داده هاست و دقیق تر می باشد\n",
    "#### augmentation\n",
    "#### به معنی افزایش اندازه دیتای تمرین است. هدف افزایش تعداد داده ها و غنی کردن دیتای تمرین استو در این جا عکس های چرخش عکس های اصلی و انعکاس عکس ها و ... را اضافه می کنیم که باعث غنی شدن داده تمرین و افزایش دقت می شود چرا که عکس های اضافه شده می توانند در عکس های تمرین نباشند ولی در تست دیده شوند"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train RF n_estimators=1000, pca=130, accuracy=100.00%\n",
      "test RF n_estimators=1000, pca=130, accuracy=49.10%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "train_imgs=pd.read_csv('CIFAR10/CIFAR10_train_data.csv',  header=None)\n",
    "train_labels=pd.read_csv('CIFAR10/CIFAR10_train_label.csv',  header=None)\n",
    "test_imgs=pd.read_csv('CIFAR10/CIFAR10_val_data.csv',  header=None)\n",
    "test_labels=pd.read_csv('CIFAR10/CIFAR10_val_label.csv',  header=None)\n",
    "\n",
    "trainData = []\n",
    "testData = []\n",
    "testLabels = test_labels.iloc[:, 0].values\n",
    "trainLabels = train_labels.iloc[:, 0].values\n",
    "for x in range(len(train_imgs.index)):\n",
    "\ttrainData.append(train_imgs.iloc[x, :].values)\n",
    "for x in range(len(test_imgs.index)):\n",
    "\ttestData.append(test_imgs.iloc[x, :].values)\n",
    "    \n",
    "# pca = PCA(n_components=130, whiten=True)\n",
    "\n",
    "# x_train_pca = pca.fit_transform(trainData)\n",
    "# x_test_pca = pca.transform(testData)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=400)\n",
    "clf.fit(trainData, trainLabels)\n",
    "score3 = clf.score(trainData, trainLabels)\n",
    "print(\"train RF n_estimators=1000, pca=130, accuracy=%.2f%%\" % (score3 * 100))\n",
    "score4 = clf.score(testData, testLabels)\n",
    "print(\"test RF n_estimators=1000, pca=130, accuracy=%.2f%%\" % (score4 * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-55288f34cfb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdependency\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhard_dependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdependency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mmissing_dependencies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdependency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0m_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0m_sanity_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m_sanity_check\u001b[0;34m()\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "i = 1\n",
    "ids = []\n",
    "res = []\n",
    "test= []\n",
    "train_imgs=pd.read_csv('CIFAR10/CIFAR10_test_data.csv',  header=None)\n",
    "for x in range(len(train_imgs.index)):\n",
    "# \tprint(train_imgs.iloc[x, :].shape)\n",
    "\ttest.append(train_imgs.iloc[x, :].values)\n",
    "for x in test:\n",
    "# \tprint(x)\n",
    "\tres.append(clf.predict(x.reshape(3072,)))\n",
    "\tids.append(i)\n",
    "\ti += 1\n",
    "result = {'id':ids, 'predict':res}\n",
    "df = DataFrame(result, columns= ['id', 'predict'])\n",
    "export_csv = df.to_csv (r'810195427.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
